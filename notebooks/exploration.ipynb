{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\an2fe\\anaconda3\\envs\\mlmodels\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn import preprocessing\n",
    "import joblib\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_classifier(X, Y, model = GradientBoostingClassifier(), random_state = None, test_size = 0.2):\n",
    "    if test_size > 0:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, Y, test_size=test_size, random_state=random_state)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = X, X.iloc[0], Y, Y.iloc[0]\n",
    "\n",
    "    #Definition and training of the model\n",
    "    gbc=model\n",
    "    X_resampled, y_resampled = X_train, y_train\n",
    "\n",
    "    gbc.fit(X_resampled, y_resampled)\n",
    "\n",
    "    if test_size > 0:\n",
    "        score = gbc.score(X_test, y_test)\n",
    "    else:    \n",
    "        score = -1\n",
    "    return gbc, score, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_loc(df, index, columns, val):\n",
    "    \"\"\" Insert data in a DataFrame with SparseDtype format\n",
    "\n",
    "    Only applicable for pandas version > 0.25\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    df : DataFrame with series formatted with pd.SparseDtype\n",
    "    index: str, or list, or slice object\n",
    "        Same as one would use as first argument of .loc[]\n",
    "    columns: str, list, or slice\n",
    "        Same one would normally use as second argument of .loc[]\n",
    "    val: insert values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df: DataFrame\n",
    "        Modified DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Save the original sparse format for reuse later\n",
    "    spdtypes = df.dtypes[columns]\n",
    "\n",
    "    # Convert concerned Series to dense format\n",
    "    df[columns] = df[columns].sparse.to_dense()\n",
    "\n",
    "    # Ensures the order of the columns is the same\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    val = val.reindex(sorted(val.columns), axis=1)\n",
    "    val_list = val.values.tolist()\n",
    "    \n",
    "    # Do a normal insertion with .loc[]\n",
    "    df.loc[index, columns] = val_list\n",
    "\n",
    "    # Back to the original sparse format\n",
    "    df[columns] = df[columns].astype(spdtypes)\n",
    "\n",
    "    return df\n",
    "\n",
    "def one_hot_encoding(df, cat_cols):\n",
    "    df_cat = df[cat_cols]\n",
    "    #df_cat = df_cat.dropna()\n",
    "    df_cat[pd.isnull(df_cat)]  = 'NaN'\n",
    "    \n",
    "    num_cols = [c for c in df.columns if c not in cat_cols]\n",
    "\n",
    "    # print(df_cat)\n",
    "    # le = preprocessing.LabelEncoder()\n",
    "    # X_2 = df_cat.apply(le.fit_transform)\n",
    "    # print(X_2)\n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "    enc.fit(df_cat)\n",
    "    onehotlabels = enc.transform(df_cat)\n",
    "    # transformed_df = pd.DataFrame(onehotlabels, columns=enc.get_feature_names_out())\n",
    "    transformed_df = pd.DataFrame.sparse.from_spmatrix(onehotlabels, columns=enc.get_feature_names_out(), index=df_cat.index)\n",
    "\n",
    "    info = {'cat_cols': cat_cols}\n",
    "    # Replace nans for distribution\n",
    "    for cat in cat_cols:\n",
    "        oh_name = [x for x in transformed_df.columns if cat in x and 'NaN' not in x]\n",
    "        # nan_df = pd.DataFrame(columns=oh_name)\n",
    "        counts = df[cat].dropna().groupby(df[cat].dropna()).count()\n",
    "        percentage = counts/len(counts)\n",
    "        percentage_df = pd.DataFrame(percentage).transpose()\n",
    "        percentage_df = percentage_df.add_prefix(cat + '_')\n",
    "        transformed_df = sp_loc(transformed_df, transformed_df[cat + '_NaN'] == 1.0, oh_name, percentage_df)\n",
    "        transformed_df = transformed_df.drop([cat + '_NaN'], axis = 1)\n",
    "        info[cat] =  percentage_df.reset_index().drop('index', axis=1)\n",
    "        #info[cat].index.name = 'index'\n",
    "    return pd.concat([df[num_cols],transformed_df], axis = 1), info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword location                                               text  target\n",
       "id                                                                            \n",
       "1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...       1\n",
       "4      NaN      NaN             Forest fire near La Ronge Sask. Canada       1\n",
       "5      NaN      NaN  All residents asked to 'shelter in place' are ...       1\n",
       "6      NaN      NaN  13,000 people receive #wildfires evacuation or...       1\n",
       "7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig = pd.read_csv(\"../data/train.csv\", index_col='id')\n",
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\an2fe\\AppData\\Local\\Temp\\ipykernel_22792\\4191983351.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cat[pd.isnull(df_cat)]  = 'NaN'\n",
      "C:\\Users\\an2fe\\AppData\\Local\\Temp\\ipykernel_22792\\4191983351.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cat[pd.isnull(df_cat)]  = 'NaN'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../data/preproc_info.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cols = ['keyword']\n",
    "ignore_cols = ['location', 'target', 'text']\n",
    "target_col = 'target'\n",
    "\n",
    "df = df_orig.copy()\n",
    "df['keyword'] = df_orig['keyword'].str.replace('%20','_')\n",
    "df, info = one_hot_encoding(df,cat_cols)\n",
    "\n",
    "data_cols = [x for x in df.columns if x not in ignore_cols]\n",
    "joblib.dump(info, \"../data/preproc_info.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "sbert_model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_embeddings = sbert_model.encode(df['text'].values.tolist())\n",
    "# joblib.dump(sentence_embeddings, \"../data/embedings_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [\"I ate dinner.\", \n",
    "#        \"We had a three-course meal.\", \n",
    "#        \"Brad came to dinner with us.\",\n",
    "#        \"He loves fish tacos.\",\n",
    "#        \"In the end, we all felt like we ate too much.\",\n",
    "#        \"We all agreed; it was a magnificent evening.\"]\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Original Sentence = \", query)\n",
    "# for sent in sentences:\n",
    "#   sim = 1 - cosine(query_vec, sbert_model.encode([sent])[0])\n",
    "#   print(\"Sentence = \", sent, \"; similarity = \", sim)\n",
    "sentence_embeddings = joblib.load(\"../data/embedings_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[data_cols].to_numpy()\n",
    "y = df[target_col].to_numpy()\n",
    "X = np.concatenate((X,sentence_embeddings), axis = 1)\n",
    "\n",
    "joblib.dump(data_cols,\"../data/data_cols.pkl\")\n",
    "joblib.dump(target_col,\"../data/target_col.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/preproc_info.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gbc, score, X_train, X_test, y_train, y_test = train_test_classifier(X, y)\n",
    "# joblib.dump(gbc, \"../data/trained_model.pkl\")\n",
    "# joblib.dump(X_train, \"../data/train_dataset.pkl\")\n",
    "# joblib.dump(X_test, \"../data/test_dataset.pkl\")\n",
    "# joblib.dump(y_train, \"../data/train_target.pkl\")\n",
    "# joblib.dump(y_test, \"../data/test_target.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "gbc = joblib.load(\"../data/trained_model.pkl\")\n",
    "\n",
    "# Load the train and test datasets\n",
    "X_train = joblib.load(\"../data/train_dataset.pkl\")\n",
    "X_test = joblib.load(\"../data/test_dataset.pkl\")\n",
    "\n",
    "# Load the train and test targets\n",
    "y_train = joblib.load(\"../data/train_target.pkl\")\n",
    "y_test = joblib.load(\"../data/test_target.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[760,  96],\n",
       "       [173, 494]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = gbc.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                     \n",
      "                                                                                \n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "                                                                                \n",
      "                                                                                \n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "A pipeline has not yet been optimized. Please call fit() first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m pipeline_optimizer \u001b[39m=\u001b[39m TPOTClassifier()\n\u001b[0;32m      2\u001b[0m pipeline_optimizer \u001b[39m=\u001b[39m TPOTClassifier(generations\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, population_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[0;32m      3\u001b[0m                                     random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, verbosity\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m pipeline_optimizer\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "File \u001b[1;32mc:\\Users\\an2fe\\anaconda3\\envs\\mlmodels\\lib\\site-packages\\tpot\\base.py:863\u001b[0m, in \u001b[0;36mTPOTBase.fit\u001b[1;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[0;32m    860\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mSystemExit\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    861\u001b[0m         \u001b[39m# raise the exception if it's our last attempt\u001b[39;00m\n\u001b[0;32m    862\u001b[0m         \u001b[39mif\u001b[39;00m attempt \u001b[39m==\u001b[39m (attempts \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m--> 863\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    864\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\an2fe\\anaconda3\\envs\\mlmodels\\lib\\site-packages\\tpot\\base.py:854\u001b[0m, in \u001b[0;36mTPOTBase.fit\u001b[1;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pbar, \u001b[39mtype\u001b[39m(\u001b[39mNone\u001b[39;00m)):\n\u001b[0;32m    852\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pbar\u001b[39m.\u001b[39mclose()\n\u001b[1;32m--> 854\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_top_pipeline()\n\u001b[0;32m    855\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_summary_of_best_pipeline(features, target)\n\u001b[0;32m    856\u001b[0m \u001b[39m# Delete the temporary cache before exiting\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\an2fe\\anaconda3\\envs\\mlmodels\\lib\\site-packages\\tpot\\base.py:961\u001b[0m, in \u001b[0;36mTPOTBase._update_top_pipeline\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    957\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_optimized_pareto_front_n_gens \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    958\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    959\u001b[0m     \u001b[39m# If user passes CTRL+C in initial generation, self._pareto_front (halloffame) shoule be not updated yet.\u001b[39;00m\n\u001b[0;32m    960\u001b[0m     \u001b[39m# need raise RuntimeError because no pipeline has been optimized\u001b[39;00m\n\u001b[1;32m--> 961\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    962\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mA pipeline has not yet been optimized. Please call fit() first.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    963\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: A pipeline has not yet been optimized. Please call fit() first."
     ]
    }
   ],
   "source": [
    "pipeline_optimizer = TPOTClassifier()\n",
    "pipeline_optimizer = TPOTClassifier(generations=20, population_size=100, cv=5,\n",
    "                                    random_state=0, verbosity=2, n_jobs=-1)\n",
    "pipeline_optimizer.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlmodels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
